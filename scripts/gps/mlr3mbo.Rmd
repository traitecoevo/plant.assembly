# mlr3mbo

A new R6 and much more modular implementation for single- and multi-objective Bayesian Optimization.

GH: https://github.com/mlr-org/mlr3mbo
Cran: https://cran.r-project.org/web/packages/mlr3mbo/index.html
Site: https://mlr3mbo.mlr-org.com/dev/articles/mlr3mbo.html#putting-it-together
Book: https://mlr3book.mlr-org.com

This package builds off a broader suite of tools, "Applied Machine Learning Using mlr3 in R" (see book https://mlr3book.mlr-org.comml3r), based on the [mlr3 package](https://github.com/mlr-org/mlr3).


Lots of sophisticated options, and focussed on black-box models

- See examples
  - Simple Optimization Example: https://github.com/mlr-org/mlr3mbo/#simple-optimization-example
  - Single-Objective: 2D Schwefel Function: https://mlr3mbo.mlr-org.com/dev/articles/mlr3mbo.html#single-objective-2d-schwefel-function
- options for 
  - multiobjetive optimisation
  - different surrogate models
  - Acquisition Functions
  - Acquisition Function Optimizer
- builds off 
  - ml3r
  - bbotk: Black-Box Optimization Toolkit 
    - https://cran.r-project.org/web/packages/bbotk/index.html
    - The package includes several optimization algorithms e.g. Random Search, Iterated Racing, Bayesian Optimization (in mlr3mbo) and Hyperband (in mlr3hyperband).
- Uses R6 classes
- higher dependency load than other packages, but perhaps less need to build intermeidate steps
- can access fitted GP after optimisation -- https://github.com/mlr-org/mlr3mbo/issues/131

More details

- Black Box Optimization: https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-black-box-optimization
- Details on learners: https://mlr3learners.mlr-org.com/
  - For GPRs using `DiceKriging::km`, for details on pars see https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.km.html
    - covtype: matern5_2	gauss, matern5_2, matern3_2, exp, powexp
    - see original paper Roustant et al 2012: doi.org/10.18637/jss.v051.i01
- different surrogates, e.g. GPs and Randomeforest both possible
  - see https://mlr3book.mlr-org.com/chapters/appendices/solutions.html#solutions-to-sec-optimization-advanced
- can paraellise calls to model
  - this will be useful when building a model of selection gradient
  - see https://mlr3book.mlr-org.com/chapters/appendices/solutions.html#solutions-to-sec-optimization-advanced


Refs

- Jones, Donald R., Matthias Schonlau, and William J. Welch. 1998. “Efficient Global Optimization of Expensive Black-Box Functions.” Journal of Global Optimization 13 (4): 455–92. https://doi.org/10.1023/A:1008306431147.

- Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, e1484.
- Bischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds. 2023. Flexible and Robust Machine Learning Using mlr3 in R. https://mlr3book.mlr-org.com.
- Roustant O, Ginsbourger D, Deville Y (2012). “DiceKriging, DiceOptim: Two R Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization.” Journal of Statistical Software, 51(1), 1--55. doi:10.18637/jss.v051.i01 .


Efficient Global Optimization of Expensive Black-Box Functions

```{r}
library(bbotk)
library(mlr3mbo)
library(mlr3learners)
library(ggplot2)
library(dplyr)
set.seed(1)

obfun <- ObjectiveRFun$new(
  fun = function(xs) list(y1 = xs$x^2),
  domain = ps(x = p_dbl(lower = -10, upper = 10)),
  codomain = ps(y1 = p_dbl(tags = "minimize"))
)

instance <- OptimInstanceSingleCrit$new(
  objective = obfun,
  terminator = trm("evals", n_evals = 10)
)

surrogate <- srlrn(lrn("regr.km", control = list(trace = FALSE)))

acqfun <- acqf("ei")

acqopt <- acqo(opt("random_search", batch_size = 100),
  terminator = trm("evals", n_evals = 100)
)

optimizer <- opt("mbo",
  loop_function = bayesopt_ego,
  surrogate = surrogate,
  acq_function = acqfun,
  acq_optimizer = acqopt
)

optimizer$optimize(instance)

# make predictions
xdt <- data.table(x = seq(-10, 10, length.out = 101))
y1 <- obfun$eval_dt(xdt)
surrogate_pred <- surrogate$predict(xdt)

xdt[, "y1" := y1]
xdt[, c("mean_prediction", "se_prediction") := surrogate_pred]

# plot true function in black
# surrogate prediction (mean +- se in grey)
# known optimum in darkred
# found optimum in darkgreen
ggplot(aes(x = x, y = y1), data = xdt) +
  geom_point(data = instance$archive$data %>% as_tibble()) +
  geom_line() +
  geom_line(aes(x = x, y = mean_prediction), colour = "grey", data = xdt) +
  geom_ribbon(aes(x = x, ymin = mean_prediction - se_prediction, ymax = mean_prediction + se_prediction), fill = "grey", alpha = 0.2) +
  geom_point(aes(x = 0, y = 0), color = "darkred") +
  geom_point(aes(x = x, y = y1), data = instance$result, color = "darkgreen") +
  theme_minimal()

```

```{r}
objective_function <- function(xs) {
  list(y = 418.9829 * 2 - (sum(unlist(xs) * sin(sqrt(abs(unlist(xs)))))))
}
domain <- ps(
  x1 = p_dbl(lower = -500, upper = 500),
  x2 = p_dbl(lower = -500, upper = 500)
)
codomain <- ps(y = p_dbl(tags = "minimize"))

objective <- ObjectiveRFun$new(
  fun = objective_function,
  domain = domain,
  codomain = codomain
)

instance <- OptimInstanceSingleCrit$new(
  objective = objective,
  search_space = domain,
  terminator = trm("evals", n_evals = 60)
)

# Gaussian Process, EI, DIRECT
#surrogate <- srlrn(default_gp()) # in example, but doesn't work with cran version

surrogate <- srlrn(lrn("regr.km", control = list(trace = FALSE)))

acq_function <- acqf("ei")
acq_optimizer <- acqo(opt("nloptr", algorithm = "NLOPT_GN_DIRECT_L"),
  terminator = trm("stagnation", threshold = 1e-8)
)
optimizer <- opt("mbo",
  loop_function = bayesopt_ego,
  surrogate = surrogate,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer
)

set.seed(2906)
optimizer$optimize(instance)

```

```
ggplot(aes(x = batch_nr, y = cummin(y)), data = instance$archive$data) +
  geom_point() +
  geom_step() +
  labs(x = "Batch Nr.", y = "Best y") +
  theme_minimal()


xdt = generate_design_grid(instance$search_space, resolution = 101)$data
ydt = objective$eval_dt(xdt)
ggplot(aes(x = x1, y = x2, z = y), data = cbind(xdt, ydt)) +
  geom_contour_filled() +
  geom_point(aes(color = batch_nr), size = 2, data = instance$archive$data) +
  scale_color_gradient(low = "lightgrey", high = "red") +
  theme_minimal()
  
```
